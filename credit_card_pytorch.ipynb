{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3a95f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "#sklearn for preprocessing and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, average_precision_score\n",
    "#matplotlib and seaborn to graph\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0bb202d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "\n",
      "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
      "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
      "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
      "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
      "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
      "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
      "\n",
      "        V26       V27       V28  Amount  Class  \n",
      "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
      "1  0.125895 -0.008983  0.014724    2.69      0  \n",
      "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
      "3 -0.221929  0.062723  0.061458  123.50      0  \n",
      "4  0.502292  0.219422  0.215153   69.99      0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "#load in dataset\n",
    "df = pd.read_csv('creditcard_fraud_detection.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5285e33e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "0dcc73d4-a68a-4fe3-976e-c2d87d11ea6c",
       "rows": [
        [
         "Time",
         "0"
        ],
        [
         "V1",
         "0"
        ],
        [
         "V2",
         "0"
        ],
        [
         "V3",
         "0"
        ],
        [
         "V4",
         "0"
        ],
        [
         "V5",
         "0"
        ],
        [
         "V6",
         "0"
        ],
        [
         "V7",
         "0"
        ],
        [
         "V8",
         "0"
        ],
        [
         "V9",
         "0"
        ],
        [
         "V10",
         "0"
        ],
        [
         "V11",
         "0"
        ],
        [
         "V12",
         "0"
        ],
        [
         "V13",
         "0"
        ],
        [
         "V14",
         "0"
        ],
        [
         "V15",
         "0"
        ],
        [
         "V16",
         "0"
        ],
        [
         "V17",
         "0"
        ],
        [
         "V18",
         "0"
        ],
        [
         "V19",
         "0"
        ],
        [
         "V20",
         "0"
        ],
        [
         "V21",
         "0"
        ],
        [
         "V22",
         "0"
        ],
        [
         "V23",
         "0"
        ],
        [
         "V24",
         "0"
        ],
        [
         "V25",
         "0"
        ],
        [
         "V26",
         "0"
        ],
        [
         "V27",
         "0"
        ],
        [
         "V28",
         "0"
        ],
        [
         "Amount",
         "0"
        ],
        [
         "Class",
         "0"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 31
       }
      },
      "text/plain": [
       "Time      0\n",
       "V1        0\n",
       "V2        0\n",
       "V3        0\n",
       "V4        0\n",
       "V5        0\n",
       "V6        0\n",
       "V7        0\n",
       "V8        0\n",
       "V9        0\n",
       "V10       0\n",
       "V11       0\n",
       "V12       0\n",
       "V13       0\n",
       "V14       0\n",
       "V15       0\n",
       "V16       0\n",
       "V17       0\n",
       "V18       0\n",
       "V19       0\n",
       "V20       0\n",
       "V21       0\n",
       "V22       0\n",
       "V23       0\n",
       "V24       0\n",
       "V25       0\n",
       "V26       0\n",
       "V27       0\n",
       "V28       0\n",
       "Amount    0\n",
       "Class     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check nas\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c4ad7e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001727485630620034\n"
     ]
    }
   ],
   "source": [
    "#calculate proportion of fraud in dataset\n",
    "print(sum(df['Class'])/len(df))\n",
    "#dataset is sparse, much more non fraud than fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bf658c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess and split for train test\n",
    "\n",
    "#X is everything but \"Class\" (PCA features, time, amount)\n",
    "X = df.drop('Class', axis = 1).values\n",
    "#y is class (0 = no fraud, 1 = fraud)\n",
    "y= df['Class'].values\n",
    "\n",
    "scalar = StandardScaler()\n",
    "#PCA features are already scaled, but time and amount aren't\n",
    "X_scaled = scalar.fit_transform(X)\n",
    "\n",
    "#train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size= 0.2, random_state= 50, \n",
    "    #stratify y since dataset is sparse and we want to ensureki\n",
    "    stratify= y\n",
    "    )\n",
    "\n",
    "#create validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "20e42ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wrap dataset in classes for pytorch\n",
    "class CreditFraudDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        #convert np arrays to tensors\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    #return len dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    #return item of X, y at specific index\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "batch_size = 2048\n",
    "\n",
    "#wrap datasets\n",
    "train_dataset = CreditFraudDataset(X_train, y_train)\n",
    "val_dataset   = CreditFraudDataset(X_val, y_val)\n",
    "test_dataset  = CreditFraudDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b5375f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FraudNet(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=30, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.3, inplace=False)\n",
      "    (8): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (9): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.3, inplace=False)\n",
      "    (12): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#set up neural net\n",
    "class FraudNet(nn.Module):\n",
    "    def __init__(self, input_dim, dropout=0.3):  # dropout is now a hyperparameter\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "\n",
    "            #layer 1, 64 neurons\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            #layer 2, 64 neurons\n",
    "            nn.Linear(64, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            #layer 3, 32 neurons\n",
    "            nn.Linear(64,32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            #only one output logit, for fraud or not\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x: (batch_size, input_dim)\n",
    "        logits = self.net(x).squeeze(1)  #(batch_size,)\n",
    "        return logits\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "#init a temporary model just to inspect the architecture\n",
    "tmp_model = FraudNet(input_dim)\n",
    "print(tmp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "25f6766b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up device\n",
    "#todo: set up cuda\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "90f1af7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up bce for model\n",
    "#count number of each class\n",
    "class_counts = np.bincount(y_train)\n",
    "neg, pos = class_counts[0], class_counts[1]\n",
    "#set weights\n",
    "pos_weight = torch.tensor([neg / pos], dtype=torch.float32).to(device)\n",
    "\n",
    "#use BCE to weight fraud more in the model. Model treats missed fraud transactions more harshly in loss function\n",
    "#without weights model can get high performance just by predicting no fraud always, since 99%+ of samples aren't fraud\n",
    "#BCE fixes this by penalizing no fraud much more harshly than no fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affbb494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_one_config(lr, dropout, batch_size, weight_decay=0.0):\n",
    "    \"\"\"\n",
    "    Train FraudNet with given hyperparameters and return the best validation PR-AUC\n",
    "    and the model state dict for that configuration.\n",
    "    \"\"\"\n",
    "    #initialize loaders (depends on batch_size)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    #set up model\n",
    "    model = FraudNet(input_dim, dropout=dropout).to(device)\n",
    "\n",
    "    #set up bce for model\n",
    "    #use BCE to weight fraud more in the model. Model treats missed fraud transactions more harshly in loss function\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_pr_auc = -np.inf\n",
    "    best_state_dict = None\n",
    "\n",
    "    num_epochs = 30 \n",
    "\n",
    "    #run training loop for each epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        #train\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(X_batch)               #output scores as logits instead of probabilities for bce\n",
    "            loss = criterion(logits, y_batch)     #calc loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        #evaluate on validation set for each epoch\n",
    "        model.eval()\n",
    "        all_logits = []\n",
    "        all_targets = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                logits = model(X_batch)\n",
    "                all_logits.append(logits.cpu())\n",
    "                all_targets.append(y_batch)\n",
    "\n",
    "        all_logits = torch.cat(all_logits)\n",
    "        all_targets = torch.cat(all_targets)\n",
    "\n",
    "        #converts logits to p\n",
    "        probs = torch.sigmoid(all_logits).numpy()\n",
    "        targets = all_targets.numpy()\n",
    "\n",
    "        #calculate roc auc score\n",
    "        roc = roc_auc_score(targets, probs)\n",
    "        #auc pr score (validation metric we care most about)\n",
    "        pr_auc = average_precision_score(targets, probs) \n",
    "\n",
    "        #track best validation PR-AUC for this config\n",
    "        if pr_auc > best_val_pr_auc:\n",
    "            best_val_pr_auc = pr_auc\n",
    "            best_state_dict = model.state_dict()\n",
    "\n",
    "        #print output for epoch (optional, you can comment this out if it's too verbose)\n",
    "        print(f\"  Epoch {epoch+1:02d} | Loss: {epoch_loss:.4f} | Val ROC-AUC: {roc:.4f} | Val PR-AUC: {pr_auc:.4f}\")\n",
    "\n",
    "    return best_val_pr_auc, best_state_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "66e4687e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter \"ranges\" for random search\n",
    "#we'll *sample* from these instead of trying every combination\n",
    "def sample_hparams():\n",
    "    #learning rate: sample log-uniform between 1e-4 and 3e-3\n",
    "    log_lr_min = math.log10(1e-4)\n",
    "    log_lr_max = math.log10(3e-3)\n",
    "    log_lr = random.uniform(log_lr_min, log_lr_max)\n",
    "    lr = 10 ** log_lr\n",
    "\n",
    "    #dropout: uniform between 0.1 and 0.5\n",
    "    dropout = random.uniform(0.1, 0.5)\n",
    "\n",
    "    #batch size: pick from a small discrete set\n",
    "    batch_size = random.choice([512, 1024, 2048])\n",
    "\n",
    "    #weight decay: small set of options (including no weight decay)\n",
    "    weight_decay = random.choice([0.0, 1e-5, 1e-4])\n",
    "\n",
    "    return {\n",
    "        \"lr\": lr,\n",
    "        \"dropout\": dropout,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"weight_decay\": weight_decay\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3956d284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Trial 1/15 ===\n",
      "Sampled hyperparameters:\n",
      "  lr          = 0.001776\n",
      "  dropout     = 0.220\n",
      "  batch_size  = 512\n",
      "  weight_decay= 0.0\n",
      "  Epoch 01 | Loss: 0.5421 | Val ROC-AUC: 0.9762 | Val PR-AUC: 0.6663\n",
      "  Epoch 02 | Loss: 0.3486 | Val ROC-AUC: 0.9816 | Val PR-AUC: 0.6746\n",
      "  Epoch 03 | Loss: 0.3466 | Val ROC-AUC: 0.9784 | Val PR-AUC: 0.6477\n",
      "  Epoch 04 | Loss: 0.2606 | Val ROC-AUC: 0.9762 | Val PR-AUC: 0.6983\n",
      "  Epoch 05 | Loss: 0.2864 | Val ROC-AUC: 0.9836 | Val PR-AUC: 0.6723\n",
      "  Epoch 06 | Loss: 0.2204 | Val ROC-AUC: 0.9825 | Val PR-AUC: 0.6821\n",
      "  Epoch 07 | Loss: 0.2189 | Val ROC-AUC: 0.9851 | Val PR-AUC: 0.6834\n",
      "  Epoch 08 | Loss: 0.2273 | Val ROC-AUC: 0.9817 | Val PR-AUC: 0.6774\n",
      "  Epoch 09 | Loss: 0.2119 | Val ROC-AUC: 0.9795 | Val PR-AUC: 0.6395\n",
      "  Epoch 10 | Loss: 0.2081 | Val ROC-AUC: 0.9828 | Val PR-AUC: 0.6750\n",
      "  Epoch 11 | Loss: 0.1678 | Val ROC-AUC: 0.9843 | Val PR-AUC: 0.7019\n",
      "  Epoch 12 | Loss: 0.1640 | Val ROC-AUC: 0.9853 | Val PR-AUC: 0.6832\n",
      "  Epoch 13 | Loss: 0.1839 | Val ROC-AUC: 0.9827 | Val PR-AUC: 0.6562\n",
      "  Epoch 14 | Loss: 0.1643 | Val ROC-AUC: 0.9834 | Val PR-AUC: 0.6690\n",
      "  Epoch 15 | Loss: 0.1445 | Val ROC-AUC: 0.9810 | Val PR-AUC: 0.6739\n",
      "  Epoch 16 | Loss: 0.1769 | Val ROC-AUC: 0.9716 | Val PR-AUC: 0.6472\n",
      "  Epoch 17 | Loss: 0.1431 | Val ROC-AUC: 0.9742 | Val PR-AUC: 0.6591\n",
      "  Epoch 18 | Loss: 0.1471 | Val ROC-AUC: 0.9813 | Val PR-AUC: 0.6797\n",
      "  Epoch 19 | Loss: 0.1727 | Val ROC-AUC: 0.9813 | Val PR-AUC: 0.6789\n",
      "  Epoch 20 | Loss: 0.1264 | Val ROC-AUC: 0.9836 | Val PR-AUC: 0.6851\n",
      "  Epoch 21 | Loss: 0.1350 | Val ROC-AUC: 0.9841 | Val PR-AUC: 0.6857\n",
      "  Epoch 22 | Loss: 0.1220 | Val ROC-AUC: 0.9842 | Val PR-AUC: 0.7032\n",
      "  Epoch 23 | Loss: 0.1247 | Val ROC-AUC: 0.9794 | Val PR-AUC: 0.6921\n",
      "  Epoch 24 | Loss: 0.1120 | Val ROC-AUC: 0.9675 | Val PR-AUC: 0.6827\n",
      "  Epoch 25 | Loss: 0.1170 | Val ROC-AUC: 0.9732 | Val PR-AUC: 0.6912\n",
      "  Epoch 26 | Loss: 0.1073 | Val ROC-AUC: 0.9763 | Val PR-AUC: 0.6967\n",
      "  Epoch 27 | Loss: 0.0978 | Val ROC-AUC: 0.9713 | Val PR-AUC: 0.6918\n",
      "  Epoch 28 | Loss: 0.1138 | Val ROC-AUC: 0.9744 | Val PR-AUC: 0.6928\n",
      "  Epoch 29 | Loss: 0.1101 | Val ROC-AUC: 0.9793 | Val PR-AUC: 0.6554\n",
      "  Epoch 30 | Loss: 0.1314 | Val ROC-AUC: 0.9848 | Val PR-AUC: 0.6995\n",
      "  Epoch 31 | Loss: 0.1117 | Val ROC-AUC: 0.9794 | Val PR-AUC: 0.6889\n",
      "  Epoch 32 | Loss: 0.0863 | Val ROC-AUC: 0.9712 | Val PR-AUC: 0.7007\n",
      "  Epoch 33 | Loss: 0.0861 | Val ROC-AUC: 0.9834 | Val PR-AUC: 0.7042\n",
      "  Epoch 34 | Loss: 0.1184 | Val ROC-AUC: 0.9774 | Val PR-AUC: 0.7029\n",
      "  Epoch 35 | Loss: 0.1025 | Val ROC-AUC: 0.9854 | Val PR-AUC: 0.7033\n",
      "  Epoch 36 | Loss: 0.0999 | Val ROC-AUC: 0.9796 | Val PR-AUC: 0.7055\n",
      "  Epoch 37 | Loss: 0.1119 | Val ROC-AUC: 0.9831 | Val PR-AUC: 0.7201\n",
      "  Epoch 38 | Loss: 0.0918 | Val ROC-AUC: 0.9849 | Val PR-AUC: 0.6958\n",
      "  Epoch 39 | Loss: 0.0922 | Val ROC-AUC: 0.9808 | Val PR-AUC: 0.6950\n",
      "  Epoch 40 | Loss: 0.1511 | Val ROC-AUC: 0.9905 | Val PR-AUC: 0.7104\n",
      "  Epoch 41 | Loss: 0.0929 | Val ROC-AUC: 0.9855 | Val PR-AUC: 0.7163\n",
      "  Epoch 42 | Loss: 0.0733 | Val ROC-AUC: 0.9920 | Val PR-AUC: 0.6930\n",
      "  Epoch 43 | Loss: 0.0718 | Val ROC-AUC: 0.9858 | Val PR-AUC: 0.7066\n",
      "  Epoch 44 | Loss: 0.0713 | Val ROC-AUC: 0.9807 | Val PR-AUC: 0.6940\n",
      "  Epoch 45 | Loss: 0.1024 | Val ROC-AUC: 0.9885 | Val PR-AUC: 0.6972\n",
      "  Epoch 46 | Loss: 0.0792 | Val ROC-AUC: 0.9893 | Val PR-AUC: 0.6996\n",
      "  Epoch 47 | Loss: 0.0778 | Val ROC-AUC: 0.9838 | Val PR-AUC: 0.6829\n",
      "  Epoch 48 | Loss: 0.0832 | Val ROC-AUC: 0.9764 | Val PR-AUC: 0.6952\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[118], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  weight_decay= \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_decay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#train and evaluate this config on the validation set\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m val_pr_auc, state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_eval_one_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--> Best val PR-AUC for this trial: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_pr_auc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m#keep track of overall best config\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[116], line 27\u001b[0m, in \u001b[0;36mtrain_eval_one_config\u001b[1;34m(lr, dropout, batch_size, weight_decay)\u001b[0m\n\u001b[0;32m     24\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     25\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 27\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jolly\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 732\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    738\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\jolly\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:788\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    787\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 788\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    789\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    790\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\jolly\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jolly\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jolly\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 212\u001b[0m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jolly\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\jolly\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#number of random configs to try\n",
    "num_trials = 15  # increase to 20â€“30 if you have time / GPU\n",
    "\n",
    "best_hparams = None\n",
    "best_val_score = -np.inf\n",
    "best_model_state = None\n",
    "\n",
    "for trial in range(1, num_trials + 1):\n",
    "    print(f\"\\n=== Trial {trial}/{num_trials} ===\")\n",
    "\n",
    "    #sample a random hyperparameter configuration\n",
    "    hparams = sample_hparams()\n",
    "    lr           = hparams[\"lr\"]\n",
    "    dropout      = hparams[\"dropout\"]\n",
    "    batch_size   = hparams[\"batch_size\"]\n",
    "    weight_decay = hparams[\"weight_decay\"]\n",
    "\n",
    "    print(f\"Sampled hyperparameters:\")\n",
    "    print(f\"  lr          = {lr:.6f}\")\n",
    "    print(f\"  dropout     = {dropout:.3f}\")\n",
    "    print(f\"  batch_size  = {batch_size}\")\n",
    "    print(f\"  weight_decay= {weight_decay}\")\n",
    "\n",
    "    #train and evaluate this config on the validation set\n",
    "    val_pr_auc, state_dict = train_eval_one_config(\n",
    "        lr=lr,\n",
    "        dropout=dropout,\n",
    "        batch_size=batch_size,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    print(f\"--> Best val PR-AUC for this trial: {val_pr_auc:.4f}\")\n",
    "\n",
    "    #keep track of overall best config\n",
    "    if val_pr_auc > best_val_score:\n",
    "        best_val_score = val_pr_auc\n",
    "        best_hparams = hparams\n",
    "        best_model_state = state_dict\n",
    "\n",
    "print(\"\\n=== Best hyperparameters found (random search) ===\")\n",
    "print(best_hparams)\n",
    "print(\"Best validation PR-AUC:\", best_val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf67f289",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rebuild the best model and load its weights\n",
    "best_model = FraudNet(input_dim, dropout=best_hparams[\"dropout\"]).to(device)\n",
    "best_model.load_state_dict(best_model_state)\n",
    "best_model.eval()\n",
    "\n",
    "#use the best batch_size for the test loader\n",
    "test_loader  = DataLoader(test_dataset, batch_size=best_hparams[\"batch_size\"], shuffle=False)\n",
    "\n",
    "all_logits = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        logits = best_model(X_batch)\n",
    "        all_logits.append(logits.cpu())\n",
    "        all_targets.append(y_batch)\n",
    "\n",
    "all_logits = torch.cat(all_logits)\n",
    "all_targets = torch.cat(all_targets)\n",
    "\n",
    "#converts logits to p\n",
    "probs = torch.sigmoid(all_logits).numpy()\n",
    "targets = all_targets.numpy()\n",
    "\n",
    "#calculate roc auc score on the test set\n",
    "roc = roc_auc_score(targets, probs)\n",
    "#auc pr score on the test set\n",
    "pr_auc = average_precision_score(targets, probs) \n",
    "\n",
    "print(\"\\n=== Test set performance with best hyperparameters ===\")\n",
    "print(f\"Test ROC-AUC: {roc:.4f}\")\n",
    "print(f\"Test PR-AUC : {pr_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0930695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[56353   511]\n",
      " [   10    88]]\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9998    0.9910    0.9954     56864\n",
      "         1.0     0.1469    0.8980    0.2525        98\n",
      "\n",
      "    accuracy                         0.9909     56962\n",
      "   macro avg     0.5734    0.9445    0.6240     56962\n",
      "weighted avg     0.9984    0.9909    0.9941     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#threshold for confusion matrix\n",
    "threshold = 0.4\n",
    "#make predictions\n",
    "preds = (probs >= threshold).astype(int)\n",
    "\n",
    "#print confusion matrix\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(targets, preds))\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(targets, preds, digits=4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
